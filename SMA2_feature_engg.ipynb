{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "93fc514d-b940-4540-995f-95b3779d11c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\users\\user\\anaconda3\\lib\\site-packages (1.5.3)\n",
      "Requirement already satisfied: numpy in c:\\users\\user\\anaconda3\\lib\\site-packages (1.23.5)\n",
      "Requirement already satisfied: matplotlib in c:\\users\\user\\appdata\\roaming\\python\\python39\\site-packages (3.7.1)\n",
      "Requirement already satisfied: seaborn in c:\\users\\user\\anaconda3\\lib\\site-packages (0.12.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\user\\anaconda3\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: nltk in c:\\users\\user\\anaconda3\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: gensim in c:\\users\\user\\anaconda3\\lib\\site-packages (4.3.1)\n",
      "Requirement already satisfied: textblob in c:\\users\\user\\anaconda3\\lib\\site-packages (0.19.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (4.39.3)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (24.1)\n",
      "Requirement already satisfied: pillow>=6.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (10.4.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (3.0.9)\n",
      "Requirement already satisfied: importlib-resources>=3.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from matplotlib) (5.12.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (1.10.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: click in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (7.1.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (2023.3.23)\n",
      "Requirement already satisfied: tqdm in c:\\users\\user\\anaconda3\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\users\\user\\anaconda3\\lib\\site-packages (from gensim) (7.1.0)\n",
      "Requirement already satisfied: zipp>=3.1.0 in c:\\users\\user\\anaconda3\\lib\\site-packages (from importlib-resources>=3.2.0->matplotlib) (3.17.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\user\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: wrapt in c:\\users\\user\\anaconda3\\lib\\site-packages (from smart-open>=1.8.1->gensim) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\user\\anaconda3\\lib\\site-packages (from tqdm->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install pandas numpy matplotlib seaborn scikit-learn nltk gensim textblob\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "addd8bc8-6f8e-43de-85a5-c21f62920923",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\User\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 44559 entries, 0 to 44558\n",
      "Data columns (total 15 columns):\n",
      " #   Column            Non-Null Count  Dtype  \n",
      "---  ------            --------------  -----  \n",
      " 0   id                44559 non-null  object \n",
      " 1   title             44559 non-null  object \n",
      " 2   text              44559 non-null  object \n",
      " 3   created_utc       44559 non-null  float64\n",
      " 4   score             44559 non-null  int64  \n",
      " 5   num_comments      44559 non-null  int64  \n",
      " 6   subreddit         44559 non-null  object \n",
      " 7   category          44559 non-null  object \n",
      " 8   disorder          44559 non-null  object \n",
      " 9   clean_text        44413 non-null  object \n",
      " 10  disorder_encoded  44559 non-null  int64  \n",
      " 11  negative          44559 non-null  float64\n",
      " 12  neutral           44559 non-null  float64\n",
      " 13  positive          44559 non-null  float64\n",
      " 14  compound          44559 non-null  float64\n",
      "dtypes: float64(5), int64(3), object(7)\n",
      "memory usage: 5.1+ MB\n",
      "None\n",
      "        id                                              title  \\\n",
      "0  1joyhmu  People who were diagnosed late in life, what's...   \n",
      "1  1jp2iex               Accidentally managed my ADHD at work   \n",
      "2  1jpf7td  Does your ADHD cause you to skip and stutter y...   \n",
      "3  1jp5gle       What is a hobby that actually stuck for you?   \n",
      "4  1jpc5di  I think I figured out why I/we eat faster than...   \n",
      "\n",
      "                                                text   created_utc  score  \\\n",
      "0  For me it was my exceptional ability to make i...  1.743521e+09   1472   \n",
      "1  I (AuDHD) am at work, at a no-phone-calls offi...  1.743531e+09    477   \n",
      "2  Sometimes I just cannot get words out at all.....  1.743564e+09     67   \n",
      "3  A common experience for people with adhd is ho...  1.743538e+09    144   \n",
      "4  So I noticed while I was eating dinner just no...  1.743556e+09     39   \n",
      "\n",
      "   num_comments subreddit  category disorder  \\\n",
      "0           793      ADHD  clinical     ADHD   \n",
      "1            82      ADHD  clinical     ADHD   \n",
      "2            34      ADHD  clinical     ADHD   \n",
      "3           411      ADHD  clinical     ADHD   \n",
      "4           153      ADHD  clinical     ADHD   \n",
      "\n",
      "                                          clean_text  disorder_encoded  \\\n",
      "0  exceptional ability make intricate highly deta...                 0   \n",
      "1  audhd work nophonecalls office job listen musi...                 0   \n",
      "2  sometimes word always using filler umm ahhh ra...                 0   \n",
      "3  common experience people adhd hobby jumping tr...                 0   \n",
      "4  noticed eating dinner dont go food utensilchew...                 0   \n",
      "\n",
      "   negative  neutral  positive  compound  \n",
      "0     0.072    0.769     0.159    0.3400  \n",
      "1     0.192    0.654     0.154   -0.6705  \n",
      "2     0.000    1.000     0.000    0.0000  \n",
      "3     0.000    0.794     0.206    0.8322  \n",
      "4     0.032    0.889     0.079    0.4939  \n",
      "Extracting text features...\n",
      "Generating Word2Vec embeddings...\n",
      "Performing topic modeling...\n",
      "Ensuring non-negative values for feature selection...\n",
      "Performing advanced feature selection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\User\\anaconda3\\lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:776: UserWarning: k=300 is greater than n_features=109. All the features will be returned.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Advanced Feature Engineering complete! Dataset saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from sklearn.pipeline import Pipeline\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from gensim.models import Word2Vec\n",
    "from textblob import TextBlob\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "file_path = r'C:\\Users\\User\\Downloads\\SMA2\\reddit_mental_health_dataset_processed_senti.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "print(df.info())\n",
    "print(df.head())\n",
    "\n",
    "def extract_features(text):\n",
    "    if isinstance(text, float):  \n",
    "        return 0, 0, 0, 0  \n",
    "    tokens = word_tokenize(text)\n",
    "    num_words = len(tokens)\n",
    "    avg_word_len = np.mean([len(word) for word in tokens]) if num_words > 0 else 0\n",
    "    sentiment = TextBlob(text).sentiment.polarity + 1  # Shift sentiment to positive range\n",
    "    lexical_richness = len(set(tokens)) / num_words if num_words > 0 else 0\n",
    "    return num_words, avg_word_len, sentiment, lexical_richness\n",
    "\n",
    "print(\"Extracting text features...\")\n",
    "df[['num_words', 'avg_word_len', 'sentiment', 'lexical_richness']] = df['clean_text'].apply(lambda x: pd.Series(extract_features(x)))\n",
    "\n",
    "print(\"Generating Word2Vec embeddings...\")\n",
    "sentences = [word_tokenize(str(text)) if isinstance(text, str) else [] for text in df['clean_text']]\n",
    "word2vec_model = Word2Vec(sentences, vector_size=100, window=5, min_count=2, workers=4)\n",
    "embedding_features = np.array([np.mean([word2vec_model.wv[word] for word in words if word in word2vec_model.wv] or [np.zeros(100)], axis=0) for words in sentences])\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "embedding_features = scaler.fit_transform(embedding_features)\n",
    "\n",
    "print(\"Performing topic modeling...\")\n",
    "lda_model = LatentDirichletAllocation(n_components=5, random_state=42)\n",
    "lda_features = lda_model.fit_transform(embedding_features)\n",
    "\n",
    "print(\"Ensuring non-negative values for feature selection...\")\n",
    "embedding_features = np.where(embedding_features < 0, 0, embedding_features)\n",
    "lda_features = np.where(lda_features < 0, 0, lda_features)\n",
    "df[['num_words', 'avg_word_len', 'sentiment', 'lexical_richness']] = np.where(df[['num_words', 'avg_word_len', 'sentiment', 'lexical_richness']] < 0, 0, df[['num_words', 'avg_word_len', 'sentiment', 'lexical_richness']])\n",
    "\n",
    "combined_features = np.hstack((embedding_features, lda_features, df[['num_words', 'avg_word_len', 'sentiment', 'lexical_richness']].values))\n",
    "\n",
    "print(\"Performing advanced feature selection...\")\n",
    "selector = SelectKBest(chi2, k=300)\n",
    "X_selected = selector.fit_transform(combined_features, df['disorder_encoded'])\n",
    "\n",
    "df.to_csv('reddit_mental_health_dataset_fengg1.csv', index=False)\n",
    "print(\"Advanced Feature Engineering complete! Dataset saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd47c89-14ba-4e7a-95f6-8b55f087348a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
